{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think Bayes, Second Edition\n",
    "\n",
    "Copyright 2020 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we're running on Colab, install empiricaldist\n",
    "# https://pypi.org/project/empiricaldist/\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install empiricaldist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get utils.py\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('utils.py'):\n",
    "    !wget https://github.com/AllenDowney/ThinkBayes2/raw/master/code/soln/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import set_pyplot_params\n",
    "set_pyplot_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces two related topics: log odds and logistic regression.\n",
    "\n",
    "In Chapter 5, we rewrote Bayes's Theorem in terms of odds and derived Bayes's Rule, which can be a convenient way to do a Bayesian update on paper or in your head.\n",
    "In this chapter, we'll look at Bayes's Rule on a logarithmic scale, which provides insight into how we accumulate evidence through successive updates.\n",
    "\n",
    "That leads directly to logistic regression, which is based on a linear model of the relationship between evidence and the log odds of a hypothesis.  \n",
    "We'll use data from the Space Shuttle to explore the relationship between temperature and the probability of damage to the O-rings.  Then we'll use data from the General Social Survey to explore generational changes in support for legalizing marijuana.  \n",
    "\n",
    "As an exercise, you'll have a chance to model the relationship between a child's age when they start school and their probability of being diagnosed with Attention Deficit Hyperactivity Disorder (ADHD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log odds\n",
    "\n",
    "When I was in grad school, I signed up for a class on Theory of Computation. \n",
    "On the first day of class, I was the first to arrive. \n",
    "A few minutes later, another student arrived. \n",
    "Because most students in the computer science program were male, I was mildly surprised that the other student was female.\n",
    "\n",
    "Another female student arrived a few minutes later, which was sufficiently surprising that I started to think I was in the wrong room. \n",
    "When third female student arrived, I was confident I was in the wrong place.\n",
    "As it turned out, I was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each student arrived, I used the observed data to update my belief that I was in the right place. We can use Bayes's Rule, which we saw in Chapter 5, to quantify the calculation I was doing intuitively.\n",
    "\n",
    "I'll use $H$ to represent the hypothesis that I was in the right room, and $F$ to represent the observation that the first other student was female. Bayes's Rule gives us:\n",
    "\n",
    "$$O(H|F) = O(H) \\frac{P(F|H)}{P(F|not H)}$$\n",
    "\n",
    "Before I saw the other students, I was confident I was in the right room, so I might assign prior odds of 10:1 in favor:\n",
    "\n",
    "$$O(H) = 10$$\n",
    "\n",
    "At the time, advanced computer science classes were about 90% male, so if I was in the right room, the likelihood of the first female student was only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I was not in the right room, the likelihood of the first female student was more like 50%, so the likelihood ratio is 1/5.  Applying Bayes's Rule:\n",
    "\n",
    "$$O(H|F) = O(H) / 5 = 2$$\n",
    "\n",
    "After the first student, the posterior odds are 2:1 I was in the right room.  \n",
    "After two students:\n",
    "\n",
    "$$O(H|FF) = O(H) / 25 = 2/5$$\n",
    "\n",
    "The posterior odds were 2/5, and after three students:\n",
    "\n",
    "$$O(H|FFF) = 2/25$$\n",
    "\n",
    "The posterior odds were 2/25.\n",
    "At that point, I was right to believe I was in the wrong room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what those odds look like in terms of probability.\n",
    "Here's the function we used in Chapter 6 to convert odds to probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(o):\n",
    "    return o / (o+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the odds after each update, the corresponding probabilities, and the change in probability after each step, expressed in percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index = ['prior', '1 student', '2 students', '3 students']\n",
    "\n",
    "table = pd.DataFrame(index=index)\n",
    "table['odds'] = [10, 2, 2/5, 2/25]\n",
    "table['prob'] = prob(table['odds'])\n",
    "table['prob diff'] = table['prob'].diff() * 100\n",
    "\n",
    "table.fillna('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each update uses the same likelihood, but the changes in probability are not the same.  The first update decreases the probability by about 24 percentage points, the second by 38, and the third by 21.\n",
    "\n",
    "That's normal for this kind of update, and in fact it's necessary; if the changes were the same size, we would quickly get negative probabilities.\n",
    "\n",
    "The odds follow a more obvious pattern.  Because each update multiplies the odds by the same likelihood ratio, the odds form a geometric sequence.\n",
    "\n",
    "And that brings us to consider another way to represent uncertainty: **log odds**, which is the logarithm of odds, usually expressed using the natural log (base $e$).\n",
    "\n",
    "Adding log odds to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "table['log odds'] = np.log(table['odds'])\n",
    "table['log odds diff'] = table['log odds'].diff()\n",
    "\n",
    "table.fillna('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice:\n",
    "\n",
    "* When probability is greater than 0.5, odds are greater than 1, and log odds are positive.\n",
    "\n",
    "* When probability is less than 0.5, odds are less than 1, and log odds are negative.\n",
    "\n",
    "You might also notice that the log odds are equally spaced.\n",
    "The change in log odds after each update is the logarithm of the likelihood ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's true in this example, and we can show that it's true in general by taking the log of both sides of Bayes's Rule.\n",
    "\n",
    "$$\\log O(H|F) = \\log O(H) + \\log \\frac{P(F|H)}{P(F|not H)}$$\n",
    "\n",
    "On a log odds scale, a Bayesian update is additive.  So if $F^x$ means that $x$ female students arrive while I am waiting, the posterior log odds that I am in the right room are:\n",
    "\n",
    "$$\\log O(H|F^x) = \\log O(H) + x \\log \\frac{P(F|H)}{P(F|not H)}$$\n",
    "\n",
    "This equation represents a linear relationship between the evidence and the posterior log odds.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the linear equation is correct, but even when it's not, it is common to use a linear function to model the relationship between an explanatory variable, $x$, and a dependent variable expressed in log odds, like this:\n",
    "\n",
    "$$\\log O(H | x) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are unknown parameters:\n",
    "\n",
    "* The intercept, $\\beta_0$, is the log odds of the hypothesis when $x$ is 0.\n",
    "\n",
    "* The slope, $\\beta_1$, is the log of the likelihood ratio.\n",
    "\n",
    "This equation is the basis of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Space Shuttle problem\n",
    "\n",
    "As an example of logistic regression, I'll solve a problem from Cameron Davidson-Pilon's book, [Bayesian Methods for Hackers](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC2.ipynb).  He writes:\n",
    "\n",
    "> \"On January 28, 1986, the twenty-fifth flight of the U.S. space shuttle program ended in disaster when one of the rocket boosters of the Shuttle Challenger exploded shortly after lift-off, killing all seven crew members. The presidential commission on the accident concluded that it was caused by the failure of an O-ring in a field joint on the rocket booster, and that this failure was due to a faulty design that made the O-ring unacceptably sensitive to a number of factors including outside temperature. Of the previous 24 flights, data were available on failures of O-rings on 23 (one was lost at sea), and these data were discussed on the evening preceding the Challenger launch, but unfortunately only the data corresponding to the 7 flights on which there was a damage incident were considered important and these were thought to show no obvious trend.\"\n",
    "\n",
    "The dataset is originally from [this paper](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1989.10478858), but also available from [Davidson-Pilon](https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('challenger_data.csv'):\n",
    "    !wget https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll read the data and do a little cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('challenger_data.csv', parse_dates=[0])\n",
    "\n",
    "# avoiding column names with spaces\n",
    "data.rename(columns={'Damage Incident': 'Damage'}, inplace=True)\n",
    "\n",
    "# dropping row 3, in which Damage Incident is NaN,\n",
    "# and row 24, which is the record for the Challenger\n",
    "data.drop(labels=[3, 24], inplace=True)\n",
    "\n",
    "# sort by temperature\n",
    "data.sort_values(by='Temperature', inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the regression model, I'll add another column to the table that contains the `Damage` column converted to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = (data['Damage'] == '1').astype(int)\n",
    "data['y'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 flights in the dataset with damage incidents.\n",
    "\n",
    "The following figure shows the relationship between damage and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import decorate\n",
    "\n",
    "def plot_data(data):\n",
    "    \"\"\"Plot damage as a function of temperature.\n",
    "    \n",
    "    data: DataFrame\n",
    "    \"\"\"\n",
    "    plt.plot(data['Temperature'], data['y'], 'o', \n",
    "             label='data', color='C0', alpha=0.4)\n",
    "\n",
    "    decorate(ylabel=\"Probability of damage\",\n",
    "         xlabel=\"Outside temperature (deg F)\",\n",
    "         title=\"Damage to O-Rings vs Temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the outside temperature was below 65 degrees, there was always damage to the O-rings.  When the temperature was above 65 degrees, there was usually no damage.  \n",
    "\n",
    "Based on this figure, it seems plausible that the probability of damage is related to temperature.  If we assume this probability follows a logistic model, we can write:\n",
    "\n",
    "$$\\log O(H | x) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $H$ is the hypothesis that the O-rings will be damaged, $x$ is temperature, and $\\beta_0$ and $\\beta_1$ are the parameters we will estimate. \n",
    "\n",
    "For reasons I'll explain soon, I'll define $x$ to be temperature shifted by an offset so its mean is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = data['Temperature'].mean()\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x'] = data['Temperature'] - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing a Bayesian update, I'll use StatsModels to run a conventional (non-Bayesian) logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula = 'y ~ x'\n",
    "results = smf.logit(formula, data=data).fit(disp=False)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results` contains a \"point estimate\" for each parameter, that is, a single value rather than a posterior distribution.\n",
    "\n",
    "The intercept is about -1.1, and the estimated slope is about -0.23.\n",
    "To see what these parameters mean, I'll use them to compute probabilities for a range of temperatures.\n",
    "\n",
    "Here's the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = results.params['Intercept']\n",
    "slope = results.params['x']\n",
    "xs = np.arange(53, 83) - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the logistic regression equation to compute log odds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds = inter + slope * xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then convert to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = np.exp(log_odds)\n",
    "ps = odds / (odds + 1)\n",
    "ps.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting log odds to probabilities is a common enough operation that it has a name, `expit`, and SciPy provides a function that computes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "ps = expit(inter + slope * xs)\n",
    "ps.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the logistic model looks like with these estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs+offset, ps, label='model', color='C1')\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At low temperatures, the probability of damage is high; at high temperatures, it drops off to near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "Now, to do a Bayesian update, we have to compute the likelihood function for possible pairs of parameters.  \n",
    "\n",
    "To demonstrate the process, let's assume temporarily that the parameters we just estimated are correct.\n",
    "I'll use the model to compute the probability of damage for each launch temperature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = data['x']\n",
    "ps = expit(inter + slope * xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ps` contains the probability of damage for each launch, according to the model.\n",
    "\n",
    "And `ys` contains boolean values that indicate whether damage occurred or not.\n",
    "\n",
    "So we can use `np.where` to compute the probability of each point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = data['y']\n",
    "likes = np.where(ys, ps, 1-ps)\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each launch where damage occurred, we choose an element from `ps`, which contains the probabilities of damage.\n",
    "\n",
    "For each launch where damage did not occur, we choose a value from the complement of `ps`, which contains the probabilities of no damage.\n",
    "\n",
    "The result is an array of probabilities, one for each launch.\n",
    "The likelihood of the whole dataset is the product of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how we compute the likelihood of the data for a particular pair of parameters.\n",
    "Now we have to compute the likelihood of the data for all possible pairs of parameters.\n",
    "To do that, we need prior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior distribution\n",
    "\n",
    "I'll use uniform distributions for both parameters, using the point estimates from the previous section to help me choose the upper and lower bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(-5, 1, num=61)\n",
    "prior_inter = make_uniform(qs, 'Intercept')\n",
    "prior_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(-0.8, 0.1, num=91)\n",
    "prior_slope = make_uniform(qs, 'Slope')\n",
    "prior_slope.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `make_joint` to construct the joint prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_joint\n",
    "\n",
    "joint = make_joint(prior_inter, prior_slope)\n",
    "joint.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of `slope` run down the rows; the values of `intercept` run across the columns.\n",
    "\n",
    "For this problem, it will be convenient to \"stack\" the prior so the parameters are levels in a `MultiIndex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from empiricaldist import Pmf\n",
    "\n",
    "joint_pmf = Pmf(joint.stack())\n",
    "joint_pmf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joint_pmf` is a `Pmf` with two levels in the index, one for each parameter.  That makes it easy to loop through possible pairs of parameters, as in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_logistic(joint_pmf, data):\n",
    "    \"\"\"Likelihood of the data for each pair of parameters.\n",
    "    \n",
    "    joint_pmf: Pmf with a two-level index\n",
    "    data: DataFrame with columns `x` and `y`\n",
    "    \n",
    "    returns: DataFrame, same shape as joint_pmf\n",
    "    \"\"\"\n",
    "    likelihood = joint_pmf.copy()\n",
    "\n",
    "    ys = data['y']\n",
    "    xs = data['x']\n",
    "\n",
    "    for slope, inter in joint_pmf.index:\n",
    "        ps = expit(inter + slope * xs)\n",
    "        likes = np.where(ys, ps, 1-ps)\n",
    "        likelihood[slope, inter] = likes.prod()\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`likelihood_logistic` takes as arguments `joint_pmf`, which is a `Pmf` that represents the joint prior distribution, and `data`, which is a `DataFrame` with columns `x` and `y`.\n",
    "\n",
    "It returns `likelihood`, which is a `Pmf` that contains the likelihood of the data for each possible pair of parameters.\n",
    "To initialize `likelihood`, it makes a copy of `joint_pmf`, which is a convenient way to make sure that `likelihood` has the same type, index, and data type as `joint_pmf`.\n",
    "\n",
    "The loop iterates through the parameters.  For each possible pair, it uses the logistic model to compute `ps`, then computes the likelihood of the data.\n",
    "\n",
    "The last line of the loop assigns each likelihood to a row in `likelihood`.\n",
    "\n",
    "Here's how we use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = likelihood_logistic(joint_pmf, data)\n",
    "likelihood.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Update\n",
    "\n",
    "Now `likelihood` is a `Pmf` with the same index as `joint_pmf`, so we can compute the posterior distribution in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_pmf = joint_pmf * likelihood\n",
    "posterior_pmf.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we used a uniform prior, the parameter pair with the highest likelihood is also the pair with maximum posterior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(posterior_pmf.max_prob(),\n",
    "          index=['slope', 'inter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can confirm that the results of the Bayesian update are consistent with the maximum likelihood estimate computed by StatsModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are approximately the same, within the precision of the grid we're using.\n",
    "\n",
    "If we unstack the posterior `Pmf` we can make a contour plot of the joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_contour\n",
    "\n",
    "joint_posterior = posterior_pmf.unstack()\n",
    "plot_contour(joint_posterior)\n",
    "decorate(title='Joint posterior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ovals in the contour plot are aligned along a diagonal, which indicates that there is some correlation between `slope` and `inter` in the posterior distribution.\n",
    "\n",
    "But the correlation is weak, which is one of the reasons we subtracted off the mean launch temperature when we computed `x`; centering the data minimizes the correlation between the parameters.\n",
    "\n",
    "To see why this matters, go back and set `offset=60` and run the analysis again.  You'll have to adjust the prior distribution of `intercept`.\n",
    "\n",
    "The slope should be the same, but the intercept will be different.  And if you plot the joint distribution, the contours you get will be elongated, indicating stronger correlation between the estimated parameters.  \n",
    "\n",
    "In theory, this correlation is not a problem, but in practice it is.  With uncentered data, the posterior distribution is more spread out, so it's harder to cover with the joint prior distribution.\n",
    "Centering the data maximizes the precision of the estimates; with uncentered data, we have to do more computation to get the same precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Marginal distributions\n",
    "\n",
    "Finally, we can extract the marginal distributions.\n",
    "Here's the posterior distribution of `inter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import marginal\n",
    "\n",
    "marginal_inter = marginal(joint_posterior, 0)\n",
    "marginal_inter.plot(color='C4')\n",
    "\n",
    "decorate(xlabel='Intercept',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior marginal distribution of intercept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the posterior distribution of `slope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_slope = marginal(joint_posterior, 1)\n",
    "marginal_slope.plot(color='C2')\n",
    "\n",
    "decorate(xlabel='Slope',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior marginal distribution of slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the posterior means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([marginal_inter.mean(), marginal_slope.mean()],\n",
    "          index=['slope', 'inter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the MAP, the posterior means are close to the parameters we estimated with StatsModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both marginal distributions are moderately skewed, so the posterior means are somewhat different from the most likely values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret these parameters.  Recall that the intercept is the log odds of the hypothesis when $x$ is 0, which is when temperature is about 70 degrees F (the value of `offset`).\n",
    "\n",
    "So we can interpret the quantities in `marginal_inter` as log odds.\n",
    "\n",
    "To convert them to probabilities, I'll use the following function, which transforms the quantities in a `Pmf` by applying a given function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace this with Pmf.transform\n",
    "\n",
    "def transform(pmf, func):\n",
    "    \"\"\"Transform the quantities in a Pmf.\n",
    "    \n",
    "    pmf: Pmf object\n",
    "    func: function object\n",
    "    \n",
    "    returns: Pmf\n",
    "    \"\"\"\n",
    "    ps = pmf.ps\n",
    "    qs = func(pmf.qs)\n",
    "    return Pmf(ps, qs, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call `transform` and pass `expit` as a parameter, it transforms the log odds in `marginal_inter` into probabilities and returns the posterior distribution of `inter` expressed in terms of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_probs = transform(marginal_inter, expit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the posterior distribution for the probability of damage at 70 degrees F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "marginal_probs.plot(color='C1')\n",
    "\n",
    "decorate(xlabel='Probability of damage at 70 deg F',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior marginal distribution of probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of this distribution is about 24%, which is the probability of damage at 70 degrees F, according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prob = marginal_probs.mean()\n",
    "mean_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows the second reason I defined `x` to be zero when temperature is 70 degrees F; this way, the intercept corresponds to the probability of damage at a relevant temperature, rather than 0 degrees F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look more closely at the estimated slope.  In the logistic model, the parameter $\\beta_1$ is the log of the likelihood ratio.  \n",
    "\n",
    "So we can interpret the quantities in `marginal_slope` as log likelihood ratios, and we can use `exp` to transform them to likelihood ratios (also known as Bayes factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_lr = transform(marginal_slope, np.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the posterior distribution of likelihood ratios; here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_lr.plot(color='C3')\n",
    "\n",
    "decorate(xlabel='Likelihood ratio of 1 deg F',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior marginal distribution of likelihood ratios')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of this distribution is about 0.75:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr = marginal_lr.mean()\n",
    "mean_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that each additional degree Fahrenheit provides evidence against the possibility of damage, with a likelihood ratio (Bayes factor) of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "\n",
    "* I computed the posterior mean of the probability of damage at 70 deg F by transforming the marginal distribution of the intercept to the marginal distribution of probability, and then computing the mean.\n",
    "\n",
    "* I computed the posterior mean of the likelihood ratio by transforming the marginal distribution of slope to the marginal distribution of likelihood ratios, and then computing the mean.\n",
    "\n",
    "This is the correct order of operations, as opposed to computing the posterior means first and then transforming them.  \n",
    "\n",
    "To see the difference, let's compute both values the other way around. \n",
    "Here's the posterior mean of `marginal_inter`, transformed to a probability, compared to the mean of `marginal_probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "expit(marginal_inter.mean()), marginal_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the posterior mean of `marginal_slope`, transformed to a likelihood ratio, compared to the mean `marginal_lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(marginal_slope.mean()), marginal_lr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the differences are not huge, but they can be.\n",
    "As a general rule, transform first, then compute summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive distributions\n",
    "\n",
    "In the logistic model, the parameters are interpretable, at least after transformation.  But often what we care about are predictions, not parameters.  In the Space Shuttle problem, the most important prediction is, \"What is the probability of O-ring damage if the external temperature is 31 degrees F?\"\n",
    "\n",
    "To make that prediction, I'll draw a sample of parameter pairs from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = posterior_pmf.sample(101)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array of 101 tuples, each representing a possible pair of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate predictions, I'll use a range of temperatures from 31 degrees F (the temperature when the Challenger launched) to 82 degrees F (the highest observed temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(31, 83)\n",
    "xs = temps - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop uses `xs` and the sample of parameters to construct an array of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.empty((len(sample), len(xs)))\n",
    "\n",
    "for i, (slope, inter) in enumerate(sample):\n",
    "    pred[i] = expit(inter + slope * xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has one column for each value in `xs` and one row for each element of `sample`.  To get a quick sense of what the predictions look like, we can loop through the rows and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ps in pred:\n",
    "    plt.plot(temps, ps, color='C5', alpha=0.1)\n",
    "    \n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overlapping lines in this figure give a sense of the most likely value at each temperature and the degree of uncertainty.\n",
    "\n",
    "To quantify the central tendency, I'll compute the median in each column, and to quantify the uncertainty, I'll compute a 90% credible interval.\n",
    "\n",
    "`np.percentile` computes the given percentiles; with the argument `axis=0`, it computes them for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, median, high = np.percentile(pred, [5, 50, 95], axis=0)\n",
    "median.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is three arrays containing predicted probabilities for the lower bound of the 90% CI, the median, and the upper bound of the CI.\n",
    "\n",
    "Here's what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(temps, low, high, color='C1', alpha=0.2)\n",
    "plt.plot(temps, median, color='C1')\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these results, the probability of damage to the O-rings at 80 degrees F is near 2%, but there is some uncertainty about that prediction; the upper bound of the CI is around 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = pd.Series(low, temps)\n",
    "median = pd.Series(median, temps)\n",
    "high = pd.Series(high, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80\n",
    "print(median[t], (low[t], high[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 60 degrees, the probability of damage is near 80%, but the CI is even wider, from 48% to 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 60\n",
    "print(median[t], (low[t], high[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the primary goal of the model is to predict the probability of damage at 31 degrees F, and the answer is at least 97%, and more likely to be more than 99.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 31\n",
    "print(median[t], (low[t], high[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One conclusion we might draw from these results is this:  If the people responsible for the Challenger launch had taken into account all of the data, and not just the seven damage incidents, they could have predicted that the probability of damage at 31 degrees F was nearly certain.  If they had, it seems likely they would have postponed the launch.\n",
    "\n",
    "At the same time, if they considered the previous figure, they might have realized that the model makes predictions that extend far beyond the data.  When we extrapolate like that, we have to remember not just the uncertainty quantified by the model, which we expressed as a credible interval; we also have to consider the possibility that the model itself is unreliable.\n",
    "\n",
    "This example is based on a logistic model, which assumes that each additional degree of temperature contributes the same amount of evidence in favor of (or against) the possibility of damage.  Within a narrow range of temperatures, that might be a reasonable assumption, especially if it is supported by data.  But over a wider range, and beyond the bounds of the data, reality has no obligation to stick to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generational Changes\n",
    "\n",
    "As a second example of logistic regression, we'll use data from the [General Social Survey](https://gss.norc.org/) (GSS) to describe generational changes in support for legalization of marijuana.\n",
    "\n",
    "Since 1972 the GSS has surveyed a representative sample of adults in the U.S., asking about issues like \"national spending priorities, crime and punishment, intergroup relations, and confidence in institutions\".\n",
    "\n",
    "I have selected a subset of the GSS data, resampled it to correct for stratified sampling, and made the results available in an HDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell downloads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "\n",
    "import os\n",
    "\n",
    "datafile = 'gss_eda.hdf5'\n",
    "if not os.path.exists(datafile):\n",
    "    !wget https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_eda.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = pd.read_hdf(datafile, 'gss')\n",
    "gss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `DataFrame` with one row for each respondent and one column for each variable.\n",
    "\n",
    "The primary variable we'll explore is `grass`, which encodes each respondent's answer to this question ([details here](https://gssdataexplorer.norc.org/variables/285/vshow)):\n",
    "\n",
    "> \"Do you think the use of marijuana should be made legal or not?\"\n",
    "\n",
    "This question was asked during most years of the survey starting in 1973, so it provides a useful view of changes in attitudes over almost 50 years.\n",
    "\n",
    "Here are is the distributions of responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss['grass'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value 1.0 represents \"yes\"; 2.0 represents \"no\"; `NaN` represents peope who were not asked the question and a small number of respondents who did not respond or said \"I don't know\".\n",
    "\n",
    "To explore generational changes in the responses, we will look at the level of support for legalization as a function of birth year, which is encoded in a variable called `cohort`.  Here's a summary of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss['cohort'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oldest GSS respondent was born in 1883; the youngest was born in 2000.\n",
    "\n",
    "Before we analyze this data, I will select the subset of respondents with valid data for `grass` and `cohort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = gss.dropna(subset=['grass', 'cohort']).copy()\n",
    "valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 37,000 respondents with the data we need.\n",
    "\n",
    "I'll recode the values of `grass` so `1` means yes and `0` means no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['y'] = valid['grass'].replace(2, 0)\n",
    "valid['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for this problem, I'm going to represent the data in a different format.  Rather than one row for each respondent, I am going to group the respondents by birth year and record the number of respondents in each group, `count`, and the number who support legalization, `sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = valid.groupby('cohort')['y'].agg(['sum', 'count'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    \"\"\"Plot the fraction of yes responses.\n",
    "    \n",
    "    data: DataFrame with columns `sum` and `count`\n",
    "    \"\"\"\n",
    "    fraction = data['sum'] / data['count']\n",
    "    plt.plot(data.index, fraction, 'o', \n",
    "             label='GSS data', color='C0', alpha=0.4)\n",
    "    \n",
    "    decorate(xlabel='Year of birth',\n",
    "             ylabel='Percent in favor',\n",
    "             title='Support for legal marijuana vs cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong relationship between birth year and support for legalization.  People born before 1920 are the least likely to say \"yes\"; people born after 1990 are the most likely. \n",
    "\n",
    "There are substantial departures from the long-term trend for people born in the 1950s and late 1960s.  If you want to conjecture about the causes, it might help to think about what was happening when each group turned 18.  People born in 1950 turned 18 during [the counterculture of the 1960s](https://en.wikipedia.org/wiki/Counterculture_of_the_1960s).  People born in the late 1960s turned 18 during the \"[Just Say No](https://en.wikipedia.org/wiki/Just_Say_No)\" era of the War on Drugs and the peak in the AIDS epidemic in the U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point estimates\n",
    "\n",
    "I'll use StatsModels again to generate point estimates for the slope and intercept of a logistic model.\n",
    "\n",
    "As we did with the previous problem, I'll center the values of the explanatory variable so the mean is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = valid['cohort'].mean()\n",
    "valid['x'] = valid['cohort'] - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results from StatsModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula = 'y ~ x'\n",
    "results = smf.logit(formula, data=valid).fit(disp=0)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, I'll use these parameters to estimate the probability of support in each cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = results.params['Intercept']\n",
    "slope = results.params['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll shift the birth years in `data` by `offset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x'] = data.index - offset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `expit` to compute the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = expit(inter + slope * data['x'])\n",
    "probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the model looks like with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.plot(label='Logistic model', color='C1')\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters, the model captures the long term trend in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing likelihoods\n",
    "\n",
    "Before we do the Bayesian update, let's compute the probability of the data with the estimated parameters.\n",
    "\n",
    "From the data, we know how many people there are in each group and how many of them support legalization.  From the model, we have an estimate for the probability of support in each group.\n",
    "\n",
    "So we can use the binomial distribution to compute the probability of the data given the estimated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "ks = data['sum']\n",
    "ns = data['count']\n",
    "likes = binom.pmf(ks, ns, probs)\n",
    "likes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each group `likes` contains the probability of the outcome, `k`, given the group size, `n`, and the estimated probability, `p`.\n",
    "\n",
    "The likelihood of the data is the product of these likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This likelihood is very small, for two reasons:\n",
    "\n",
    "* The dataset is large, which means that there are many possible outcomes, so the probability of *any* particular outcome is small.\n",
    "\n",
    "* The data deviate substantially from the model, so the probability of *this* particular outcome is small.\n",
    "\n",
    "In theory, it's not a problem if the likelihood of the data is small.  We might not get a model that fits the data perfectly, but we'll get the parameters that come as close as possible.\n",
    "\n",
    "However, in practice small likelihoods can be problematic.  With floating-point numbers, the smallest positive number we can represent is about `1e-1021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.float_info.min_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any number smaller than that \"underflows\"; that is, it gets rounded down to 0.  When that happens, we lose the ability to distinguish between parameters that make the model fit the data or not.  In the worst case, if all likelihoods underflow, all probabilities in the posterior distribution would be 0.\n",
    "\n",
    "In this example, the likelihoods are big enough that we can still do a Bayesian update, so we'll do that next.\n",
    "Then I will demonstrate a trick we can use to avoid underflow: computing likelihoods under a log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The update\n",
    "\n",
    "I'll use uniform priors for the parameters, with locations centered around the point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(-0.95, -0.75, num=51)\n",
    "prior_inter = make_uniform(qs, 'Intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(0.025, 0.035, num=51)\n",
    "prior_slope = make_uniform(qs, 'Slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll make a joint prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = make_joint(prior_inter, prior_slope)\n",
    "joint.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And stack it into a `Pmf` with a two-column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_pmf = Pmf(joint.stack())\n",
    "joint_pmf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the update, using the binomial distribution to compute the likelihood of the data in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = joint_pmf.copy()\n",
    "\n",
    "xs = data['x']\n",
    "ks = data['sum']\n",
    "ns = data['count']\n",
    "\n",
    "for slope, inter in joint_pmf.index:\n",
    "    ps = expit(inter + slope * xs)\n",
    "    likes = binom.pmf(ks, ns, ps)\n",
    "    likelihood[slope, inter] = likes.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the likelihoods are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do the update in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_pmf = joint_pmf * likelihood\n",
    "posterior_pmf.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are enough non-zero elements to get a useful posterior distribution.\n",
    "\n",
    "Here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior = posterior_pmf.unstack()\n",
    "\n",
    "plot_contour(joint_posterior)\n",
    "decorate(title='Joint posterior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the parameters with maximum posterior probability are consistent with the point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_pmf.max_prob())\n",
    "print(results.params.values[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the means of the marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_inter = marginal(joint_posterior, 0)\n",
    "marginal_slope = marginal(joint_posterior, 1)\n",
    "\n",
    "marginal_inter.mean(), marginal_slope.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the intercept indicates the log odds of the hypothesis at `x=0`.\n",
    "To make the distribution of intercepts easier to interpret, I'll use `expit` to transform the values to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_probs = transform(marginal_inter, expit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marginal_probs.plot(color='C4')\n",
    "decorate(xlabel='Probability at x=0',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior distribution of intercept in terms of probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of this distribution is about 24%, which is the predicted probability of supporting legalization for someone born around 1949."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_probs.mean(), offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated slope is the log of the likelihood ratio for each additional year of birth.  To interpret slopes as likelihood ratios, we can use `np.exp` to transform the values in the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_lr = transform(marginal_inter, np.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marginal_lr.plot(color='C2')\n",
    "\n",
    "decorate(xlabel='Likelihood ratio of each additional year',\n",
    "         ylabel='PDF',\n",
    "         title='Posterior distribution of slope in terms of likelihood ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of this distribution is about 0.43, which indicates that each additional year is evidence that the respondent will say \"yes\", with a a likelihood ratio (or Bayes factor) of 0.43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_lr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we will use the joint posterior distribution to generate predictions, but first I'll show how to compute likelihoods under a log transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood\n",
    "\n",
    "Because of the problem of underflow, many likelihood computations are done under a log transform.  That's why the distributions in SciPy, including `binom`, provide functions to compute logarithms of PMFs and PDFs.\n",
    "\n",
    "Here's a loop that uses `binom.logpmf` to compute the log likelihood of the data for each pair of parameters in `joint_pmf`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = joint_pmf.copy()\n",
    "\n",
    "for slope, inter in joint_pmf.index:\n",
    "    ps = expit(inter + slope * xs)\n",
    "    log_likes = binom.logpmf(ks, ns, ps)\n",
    "    log_likelihood[slope, inter] = log_likes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`log_likes` is an array that contains the logarithms of the binomial PMFs for each group.\n",
    "The sum of these logarithms is the log of their product, which is the log-likelihood of the data.\n",
    "\n",
    "Since the likelihoods are small, their logarithms are negative.  The smallest (most negative) is about -610; the largest (least negative) is about -480."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood.min(), log_likelihood.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the log likelihoods are comfortably with the range we can represent with floating-point numbers.\n",
    "\n",
    "However, before we can do the update, we have to convert the logarithms back to a linear scale.  To do that while minimizing underflow, I am going to shift the logs up toward zero.\n",
    "\n",
    "Adding a constant to the `log_likelihood` is the same as multiplying a constant by `likelihood`.  \n",
    "We can do that without affecting the results because we have to normalize the posterior probabilities, so the multiplicative constant gets normalized away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted = log_likelihood - log_likelihood.max()\n",
    "likelihood2 = np.exp(shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After subtracting away the largest element in `log_likelihood`, the range of values in the result is from -127 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted.min(), shifted.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the range of likelihoods is from near 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood2.min(), likelihood2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use them as likelihoods in a Bayesian update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_pmf2 = joint_pmf * likelihood2\n",
    "posterior_pmf2.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To confirm that we get the same results using likelihoods or log-likelihoods, I'll compute the mean of the marginal posterior distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_posterior2 = posterior_pmf2.unstack()\n",
    "\n",
    "marginal2_inter = marginal(joint_posterior2, 0)\n",
    "marginal2_slope = marginal(joint_posterior2, 1)\n",
    "\n",
    "print(marginal2_inter.mean(), marginal2_slope.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare them to what we got using (non-log) likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marginal_inter.mean(), marginal_slope.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the same except for small differences due to floating-point approximation.\n",
    "\n",
    "In this example, we can compute the posterior distribution either way, using likelihoods or log likelihoods.\n",
    "But if there were more data, the likelihoods would underflow and it would be necessary to use log likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "As we did with the previous example, we can use the posterior distribution of the parameters to generate predictions, which we can use to see whether the model fits the data and to extrapolate beyond the data.\n",
    "\n",
    "I'll start with a sample from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = posterior_pmf.sample(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a range of `xs` that extends 20 years past the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(1880, 2021) - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the sampled parameters to predict probabilities for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.empty((len(sample), len(xs)))\n",
    "\n",
    "for i, (slope, inter) in enumerate(sample):\n",
    "    ps[i] = expit(inter + slope * xs)\n",
    "    \n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that only accounts for uncertainty about the parameters.\n",
    "\n",
    "We also have to account for variability in the size of the groups.  Here's the distribution of group size, dropping the groups smaller than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_small = (data['count'] >= 20)\n",
    "counts = data.loc[not_small, 'count']\n",
    "counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate variation in group size, I'll use `np.random.choice` to resample the group sizes; that is, I'll draw from `counts` a sample with the same length as `xs`, sampling with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.random.choice(counts, len(xs), replace=True)\n",
    "ns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we know how many people are in each group and their probability of saying \"yes\", there is still uncertainty in the outcome.  We can use the binomial distribution to simulate this (final) source of uncertainty.\n",
    "\n",
    "Putting it all together, the following loop combines these sources of uncertainty to generate predictive distributions for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.empty((len(sample), len(xs)))\n",
    "\n",
    "for i, (slope, inter) in enumerate(sample):\n",
    "    ps = expit(inter + slope * xs)\n",
    "    ns = np.random.choice(counts, len(xs), replace=True)\n",
    "    ks = binom(ns, ps).rvs(len(xs))\n",
    "    pred[i] = ks / ns\n",
    "    \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with one row for each pair of parameters in the sample and one column for each value in `xs`.\n",
    "\n",
    "Now we can use `np.percentile` to compute percentiles in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, median, high = np.percentile(pred, [5, 50, 95], axis=0)\n",
    "median.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use them to plot a 90% credible interval for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.fill_between(xs+offset, low, high, \n",
    "                 color='C1', alpha=0.2)\n",
    "\n",
    "plt.plot(xs+offset, median, label='Logistic model', \n",
    "         color='C1')\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fits the long-term trend of the data, with most data points falling within the 90% credible interval despite the apparent deviations from the trend.\n",
    "\n",
    "The model predicts that people born between 2000 and 2020 will be more likely to support legalizing marijuana (when they are old enough to be respondents in the General Social Survey)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Bayes\n",
    "\n",
    "In this chapter I used StatsModels to compute the parameters that maximize the probability of the data, and then used those estimates to choose the bounds of the uniform prior distributions.\n",
    "\n",
    "It might have occurred to you that this process uses the data twice, once to choose the priors and again to do the update.  If that bothers you, you are not alone.\n",
    "\n",
    "The process I used is an example of what's called the [Empirical Bayes method](https://en.wikipedia.org/wiki/Empirical_Bayes_method), although I don't think that's a particularly good name for it.\n",
    "\n",
    "Although it might seem problematic to use the data twice, in these examples, it is not.  To see why, consider an alternative: instead of using the estimated parameters to choose the bounds of the prior distribution, I could have used uniform distributions with much wider ranges.  \n",
    "\n",
    "In that case, the results would be the same; the only difference is that I would spend more time computing likelihoods for parameters where the posterior probabilities are negligibly small.\n",
    "\n",
    "So you can think of this version of Empirical Bayes as an optimization that minimizes computation by putting the prior distributions where the likelihood of the data is worth computing.\n",
    "\n",
    "This optimization doesn't affect the results, so it doesn't \"double-count\" the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So far we have seen three ways to represent degrees of confidence in a hypothesis: probability, odds, and log odds.\n",
    "When we write Bayes's Rule in terms of log odds, a Bayesian update is the sum of the prior and the likelihood; in this sense, Bayesian statistics is the arithmetic of hypotheses and evidence.\n",
    "\n",
    "This form of Bayes Theorem is also the foundation of logistic regression, which we used to infer parameters and make predictions.  In the Space Shuttle problem, we modeled the relationship between temperature and the probability of damage, and showed that the Challenger disaster might have been predictable.  But this example is also a warning about the hazards of using a model to extrapolate far beyond the data.\n",
    "\n",
    "We also modeled generational changes in support for legalizing marijuana, which was less than 10% among people born in the 1890s, and is nearly 80% among people born in the 1990s.\n",
    "Because the dataset we used is large, the total probability of the data is very small, which can exceed the limits of floating-point arithmetic.\n",
    "We can avoid this problem by computing likelihoods under a logarithmic transformation.\n",
    "\n",
    "In the exercises below you'll have a chance to practice the material in this chapter, using log odds to evaluate a political pundit and using logistic regression to model diagnosis rates for Attention Deficit Hyperactivity Disorder (ADHD).\n",
    "\n",
    "In the next chapter we'll move from logistic regression to linear regression, which we will use to model changes over time in temperature, snowfall, and the marathon world record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise:** Suppose a political pundit claims to be able to predict the outcome of elections, but instead of picking a winner, they give each candidate a probability of winning.\n",
    "With that kind of prediction, it can be hard to say whether it is right or wrong. \n",
    "\n",
    "For example, suppose the pundit says that Alice has a 70% chance of beating Bob, and then Bob wins the election.  Does that mean the pundit was wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to answer this question is to consider two hypotheses:\n",
    "\n",
    "* `H`: The pundit's algorithm is legitimate; the probabilities it produces are correct in the sense that they accurately reflect the candidates' probabilities of winning.\n",
    "\n",
    "* `not H`: The pundit's algorithm is bogus; the probabilities it produces are random values with a mean of 50%.\n",
    "\n",
    "If the pundit says Alice has a 70% chance of winning, and she does, that provides evidence in favor of `H` with likelihood ratio 70/50.\n",
    "\n",
    "If the pundit says Alice has a 70% chance of winning, and she loses, that's evidence against `H` with a likelihood ratio of 50/30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we start with some confidence in the algorithm, so the prior odds are 4 to 1.  And suppose the pundit generates predictions for three elections:\n",
    "\n",
    "* In the first election, the pundit says Alice has a 70% of winning and she does.\n",
    "\n",
    "* In the second election, the pundit says Bob has a 30% chance of winning and he does.\n",
    "\n",
    "* In the third election, the pundit says Carol has an 90% chance of winning and she does.\n",
    "\n",
    "What is the log likelihood ratio for each of these outcomes?  Use the log-odds form of Bayes's Rule to compute the posterior log odds for `H` after these outcomes.  In total, do these outcomes increase or decrease your confidence in the pundit?\n",
    "\n",
    "If you are interested in this topic, you can [read more about it in this blog post](http://allendowney.blogspot.com/2016/11/why-are-we-so-surprised.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  An article in the New England Journal of Medicine reports results from a study that looked at the diagnosis rate of Attention Deficit Hyperactivity Disorder (ADHD) as a function of birth month: [\"Attention Deficit–Hyperactivity Disorder and Month of School Enrollment\"](https://www.nejm.org/doi/10.1056/NEJMoa1806828).\n",
    "\n",
    "They found that children born in June, July, and August were substantially more likely to be diagnosed with ADHD, compared to children born in September, but only in states that use a September cutoff for children to enter kindergarten.  In these states, children born in August start school almost a year younger than children born in September.  The authors of the study suggest that the cause is \"age-based variation in behavior that may be attributed to ADHD rather than to the younger age of the children\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper includes this figure:\n",
    "\n",
    "<img width=\"500\" src=\"https://www.nejm.org/na101/home/literatum/publisher/mms/journals/content/nejm/2018/nejm_2018.379.issue-22/nejmoa1806828/20190131/images/img_xlarge/nejmoa1806828_f1.jpeg\">\n",
    "\n",
    "In my opinion, this representation of the data does not show the effect as clearly as it could.\n",
    "\n",
    "But the figure includes the raw data, so we can analyze it ourselves.\n",
    "\n",
    "Note: there is an error in the figure, confirmed by personal correspondence:\n",
    "\n",
    "> The May and June [diagnoses] are reversed. May should be 317 (not 287) and June should be 287 (not 317).\n",
    "\n",
    "So here is the corrected data, where `n` is the number of children born in each month, starting with January, and `k` is the number of children diagnosed with ADHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.array([32690, 31238, 34405, 34565, 34977, 34415, \n",
    "                   36577, 36319, 35353, 34405, 31285, 31617])\n",
    "\n",
    "k = np.array([265, 280, 307, 312, 317, 287, \n",
    "                      320, 309, 225, 240, 232, 243])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'm going to \"roll\" the data so it starts in September rather than January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12)\n",
    "n = np.roll(n, -8)\n",
    "k = np.roll(k, -8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I'll put it in a `DataFrame` with one row for each month and the diagnosis rate per 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dict(x=x, k=k, n=n))\n",
    "data['rate'] = data['k'] / data['n'] * 10000\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the diagnosis rates look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    plt.plot(data['x'], data['rate'], 'o', \n",
    "             label='data', color='C0', alpha=0.4)\n",
    "    \n",
    "    plt.axvline(5.5, color='gray', alpha=0.2)\n",
    "    plt.text(6, 64, 'Younger than average')\n",
    "    plt.text(5, 64, 'Older than average', horizontalalignment='right')\n",
    "\n",
    "    decorate(xlabel='Birth date, months after cutoff',\n",
    "             ylabel='Diagnosis rate per 10,000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 9 months, from September to May, we see what we would expect if some of the excess diagnoses are due to \"age-based variation in behavior\".  For each month of difference in age, we see an increase in the number of diagnoses.\n",
    "\n",
    "This pattern breaks down for the last three months, June, July, and August.  This might be explained by random variation, but it also might be due to parental manipulation; if some parents hold back children born near the deadline, the observations for these month would include a mixture of children who are relatively old for their grade and therefore less likely to be diagnosed.\n",
    "\n",
    "Unfortunately, the dataset includes only month of birth, not year, so we don't know the actual ages of these students when they started school.  However, we can use the first nine months to estimate the effect of age on diagnosis rate; then we can think about what to do with the other three months.\n",
    "\n",
    "Use the methods in this chapter to estimate the probability of diagnosis as a function of birth month.  Start with the following prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(-5.2, -4.6, num=51)\n",
    "prior_inter = make_uniform(qs, 'Intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.linspace(0.0, 0.08, num=51)\n",
    "prior_slope = make_uniform(qs, 'Slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a joint prior distribution and update it using the data for the first nine months.\n",
    "\n",
    "2. Then draw a sample from the posterior distribution and use it to compute the median probability of diagnosis for each month and a 90% credible interval.\n",
    "\n",
    "3. As a bonus exercise, do a second update using the data from the last three months, but treating the observed number of diagnoses as a lower bound on the number of diagnoses there would be if no children were kept back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The GSS dataset we used to explore generational changes contains more than 100 other variables related to the attitudes and beliefs of the respondents.  You can [read about these variables here](https://gssdataexplorer.norc.org/projects/52787).  Choose one and run an analysis similar to what we did with `grass`.\n",
    "\n",
    "How well does the logistic model describe the long-term trend for the variable you chose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
