{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think Bayes, Second Edition\n",
    "\n",
    "Copyright 2020 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we're running on Colab, install empiricaldist\n",
    "# https://pypi.org/project/empiricaldist/\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install empiricaldist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get utils.py\n",
    "import os\n",
    "\n",
    "if not os.path.exists('utils.py'):\n",
    "    !wget https://github.com/AllenDowney/ThinkBayes2/raw/master/soln/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_pyplot_params\n",
    "set_pyplot_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces \"survival analysis\", which is a set of statistical methods used to answer questions about the time until an event.\n",
    "In the context of medicine it is literally about survival, but it can be applied to the time until any kind of event, or instead of time it can be about space or other dimensions.\n",
    "\n",
    "Survival analysis is challenging because the data we have are often incomplete.  But as we'll see, Bayesian methods are particularly good at working with incomplete data.\n",
    "\n",
    "As examples, we'll consider two applications that are a little less serious than life and death: the time until light bulbs fail and the time until dogs in a shelter are adopted.\n",
    "\n",
    "To describe these \"survival times\", we'll use the Weibull distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Weibull distribution\n",
    "\n",
    "The [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution) is often used in survival analysis because it is a good model for the distribution of lifetimes for manufactured products, at least over some parts of the range.\n",
    "\n",
    "SciPy provides implementations for several versions of the Weibull distribution; the one I'll use is called `weibull_min`.\n",
    "\n",
    "To make this implementation easier to use, I'll wrap it in a function that takes two parameters: $\\lambda$, which mostly affects the location or \"central tendency\" of the distribution, and $k$, which affects the shape.\n",
    "\n",
    "The parameter $\\lambda$ is pronounced \"lambda\" and represented with the variable name `lam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import weibull_min\n",
    "\n",
    "def weibull_dist(lam, k):\n",
    "    \"\"\"Makes a weibull object.\n",
    "    \"\"\"\n",
    "    return weibull_min(k, scale=lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here's a Weibull distribution with parameters $\\lambda=3$ and $k=0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 3\n",
    "k = 0.8\n",
    "actual_dist = weibull_dist(lam, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an object that represents the distribution.\n",
    "\n",
    "Here's what the Weibull CDF looks like with those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from empiricaldist import Cdf\n",
    "from utils import decorate\n",
    "\n",
    "qs = np.linspace(0, 12, 101)\n",
    "ps = actual_dist.cdf(qs)\n",
    "cdf = Cdf(ps, qs)\n",
    "cdf.plot()\n",
    "\n",
    "decorate(xlabel='Duration in time', ylabel='CDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`actual_dist` provides `rvs`, which we can use to generate a random sample from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "data = actual_dist.rvs(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given the parameters of the distribution, we can generate a sample.\n",
    "\n",
    "Now let's see if we can go the other way: given the sample, we'll estimate the parameters.\n",
    "\n",
    "Here's a uniform prior distribution for $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_uniform\n",
    "\n",
    "lams = np.linspace(0.1, 10.1, num=101)\n",
    "prior_lam = make_uniform(lams, name='lambda')\n",
    "prior_lam.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a uniform prior for $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.linspace(0.1, 5.1, num=101)\n",
    "prior_k = make_uniform(ks, name='k')\n",
    "prior_k.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I'll use `make_joint` to make a joint prior distribution for the two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_joint\n",
    "\n",
    "prior = make_joint(prior_lam, prior_k)\n",
    "prior.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `DataFrame` that represents the joint prior.\n",
    "\n",
    "Now I'll use `meshgrid` to make a 3-D mesh with $\\lambda$ on the first axis (numbered `axis=0`), $k$ on the second axis (`axis=1`), and the data on the third axis (`axis=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_mesh, k_mesh, data_mesh = np.meshgrid(\n",
    "    prior.columns, prior.index, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `weibull_dist` to compute the PDF of the Weibull distribution for each pair of parameters and each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = weibull_dist(lam_mesh, k_mesh).pdf(data_mesh)\n",
    "densities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the data is the product of the probability densities along `axis=2`, which is the axis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = densities.prod(axis=2)\n",
    "likelihood.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the posterior distribution in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import normalize\n",
    "\n",
    "posterior = prior * likelihood\n",
    "normalize(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function encapsulates these steps.  \n",
    "It takes a joint prior distribution and the data, and returns a joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weibull(prior, data):\n",
    "    \"\"\"Update the prior based on data.\n",
    "    \n",
    "    prior: joint distribution of mu and sigma\n",
    "    data: sequence of observations\n",
    "    \"\"\"\n",
    "    lam_mesh, k_mesh, data_mesh = np.meshgrid(\n",
    "        prior.columns, prior.index, data)\n",
    "    \n",
    "    densities = weibull_dist(lam_mesh, k_mesh).pdf(data_mesh)\n",
    "    likelihood = densities.prod(axis=2)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    normalize(posterior)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = update_weibull(prior, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a contour plot of the joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import plot_contour\n",
    "\n",
    "plot_contour(posterior)\n",
    "decorate(title='Posterior joint distribution of Weibull parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blob in the lower left contains almost all of the probability density; the jaggy line in the upper left is an irrelevent boundary between regions of very low probability.\n",
    "\n",
    "It looks like the range of likely values for $\\lambda$ is about 1 to 4, which contains the actual value we used to generate the data, 3.\n",
    "And the range for $k$ is about 0.5 to 1.5, which contains the actual value, 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal distributions\n",
    "\n",
    "To be more precise about these ranges, we can extract the marginal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import marginal\n",
    "\n",
    "posterior_lam = marginal(posterior, 0)\n",
    "posterior_k = marginal(posterior, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute the posterior means and 90% credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "posterior_lam.plot(color='C4')\n",
    "plt.axvline(3, color='gray')\n",
    "decorate(xlabel='lam',\n",
    "         ylabel='PDF', \n",
    "         title='Posterior marginal distribution of lam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical gray line show the actual value of $\\lambda$.\n",
    "\n",
    "And here are the posterior means and 90% credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_lam.mean(), posterior_lam.credible_interval(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the marginal posterior distribution for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_k.plot(color='C4')\n",
    "plt.axvline(0.8, color='gray')\n",
    "decorate(xlabel='k',\n",
    "         ylabel='PDF', \n",
    "         title='Posterior marginal distribution of k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the posterior mean and credible interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_k.mean(), posterior_k.credible_interval(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distributions are wide, which means that with only 10 data points we can't estimated the parameters precisely.\n",
    "But for both parameters, the actual value falls in the credible interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete Data\n",
    "\n",
    "In the previous example we were given 10 random values from a Weibull distribution, and we used them to estimate the parameters (which we pretended we didn't know).\n",
    "\n",
    "But in many real-world scenarios, we don't have complete data; in particular, when we observe a system at a point in time, we generally have information about the past, but not the future.\n",
    "\n",
    "As an example, suppose you work at a dog shelter and you are interested in the time between the arrival of a new dog and when it is adopted.\n",
    "Some dogs might be snapped up immediately; others might have to wait longer.\n",
    "The people who operate the shelter might want to make inferences about the distribution of these residence times.\n",
    "\n",
    "Suppose you monitor arrivals and departures over a 8 weeks, and 10 dogs arrive during that period.\n",
    "I'll assume that their arrival times are distributed uniformly, so I'll generate random values like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19)\n",
    "start = np.random.uniform(0, 8, size=10)\n",
    "start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's suppose that the residence times follow the Weibull distribution we used in the previous example.\n",
    "We can generate a sample from that distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "duration = actual_dist.rvs(10)\n",
    "duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use these values to construct a `DataFrame` that contains the arrival and departure times for each dog, called `start` and `end`.\n",
    "And I'll add a third column, `status`, which indicates whether the dog has been adopted; initially the status is `1` for all dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "obs = pd.DataFrame(dict(start=start,\n",
    "                        end=start+duration,\n",
    "                        status=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For display purposes, I'll sort the rows of the `DataFrame` by arrival time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs.sort_values(by='start', ignore_index=True)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots a \"lifeline\" for each dog, showing the arrival and departure times on a time line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lifelines(obs):\n",
    "    \"\"\"Plot a line for each observation.\n",
    "    \n",
    "    obs: DataFrame\n",
    "    \"\"\"\n",
    "    for y, row in obs.iterrows():\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        status = row['status']\n",
    "        \n",
    "        if status == 0:\n",
    "            # ongoing\n",
    "            plt.hlines(y, start, end, color='C0')\n",
    "        else:\n",
    "            # complete\n",
    "            plt.hlines(y, start, end, color='C1')\n",
    "            plt.plot(end, y, marker='o', color='C1')\n",
    "            \n",
    "    decorate(xlabel='Time (weeks)',\n",
    "             ylabel='Dog index')\n",
    "\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it looks like for the data we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifelines(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start times are distributed uniformly, but the lengths of the lines are highly variable.  For example, Dog 2 was adopted almost immediately; Dog 4 had to wait more than 5 weeks.\n",
    "\n",
    "But notice that several of the lifelines extend past the observation window of 8 weeks.\n",
    "So if we observed this system at the beginning of Week 8, we would have incomplete information.\n",
    "Specifically, we would not know the future adoption times for Dogs 6, 7, and 8.\n",
    "\n",
    "I'll simulate this incomplete data by identifying the lifelines that extend past the observation window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2 = obs.copy()\n",
    "censored = obs2['end'] > 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`censored` is a  Boolean Series that is `True` for lifelines that extend past Week 8.\n",
    "\n",
    "Data that is not available is sometimes called \"censored\" in the sense that it is hidden from us.\n",
    "But in this case it is hidden because we don't know the future, not because someone is censoring it.\n",
    "\n",
    "For the lifelines that are censored, I'll modify `end` to indicate when they are last observed and `status` to indicate that the observation is incomplete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2.loc[censored, 'end'] = 8\n",
    "obs2.loc[censored, 'status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the results to show both complete and incomplete lifelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifelines(obs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I'll add one more column to the table, which contains the duration of the observed parts of the lifelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2['T'] = obs2['end'] - obs2['start']\n",
    "obs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have simulated is the data that would be available at the beginning of Week 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Incomplete Data\n",
    "\n",
    "Now, let's see how we can use both kinds of data, complete and incomplete, to infer the parameters of the distribution of residence times.\n",
    "\n",
    "First I'll split the data into two sets: `data1` contains residence times for dogs whose arrival and departure times are known; `data2` contains incomplete residence times for dogs who were not adopted during the observation interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = obs2['status'] == 1\n",
    "data1 = obs2.loc[complete, 'T']\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = obs2.loc[~complete, 'T']\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complete data, we can use `update_weibull`, which uses the PDF of the Weibull distribution to compute the likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior1 = update_weibull(prior, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the incomplete data, we have to think a little harder.\n",
    "At the end of the observation interval, we don't know what the residence time will be, but we can put a lower bound on it; that is, we can say that the residence time will be greater than `T`.\n",
    "\n",
    "And that means that we can compute the likelihood of the data using the survival function, which is the probability that a value from the distribution exceeds `T`.\n",
    "\n",
    "The following function is identical to `update_weibull` except that it uses `sf`, which computes the survival function, rather than `pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weibull_incomplete(prior, data):\n",
    "    \"\"\"Update the prior based on data.\n",
    "    \n",
    "    prior: joint distribution of mu and sigma\n",
    "    data: sequence of observations\n",
    "    \"\"\"\n",
    "    lam_mesh, k_mesh, data_mesh = np.meshgrid(\n",
    "        prior.columns, prior.index, data)\n",
    "    densities = weibull_dist(lam_mesh, k_mesh).sf(data_mesh)\n",
    "    likelihood = densities.prod(axis=2)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    normalize(posterior)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the update with the incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior2 = update_weibull_incomplete(posterior1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the joint posterior distribution looks like after both updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(posterior2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous contour plot, it looks like the range of likely values for $\\lambda$ is substantially wider.\n",
    "We can see that more clearly by looking at the marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_lam2 = marginal(posterior2, 0)\n",
    "posterior_k2 = marginal(posterior2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the posterior marginal distribution for $\\lambda$ compared to the distribution we got using all complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_lam.plot(color='C4', label='All complete')\n",
    "posterior_lam2.plot(color='C2', label='Some censored')\n",
    "\n",
    "decorate(xlabel='lambda',\n",
    "         ylabel='PDF', \n",
    "         title='Marginal posterior distribution of lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution with some incomplete data is substantially wider.\n",
    "\n",
    "As an aside, notice that the posterior distribution does not come all the way to 0 on the right side.\n",
    "That suggests that the range of the prior distribution is not wide enough to cover the most likely values for this parameter.\n",
    "If I were concerned about making this distribution more accurate, I would go back and run the update again with a wider prior.\n",
    "\n",
    "Here's the posterior marginal distribution for $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_k.plot(color='C4', label='All complete')\n",
    "posterior_k2.plot(color='C2', label='Some censored')\n",
    "\n",
    "decorate(xlabel='k',\n",
    "         ylabel='PDF', \n",
    "         title='Posterior marginal distribution of k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the marginal distribution is shifted to the left when we have incomplete data, but it is not substantially wider.\n",
    "\n",
    "In summary, we have seen how to combine complete and incomplete data to estimate the parameters of a Weibull distribution, which is useful in many real-world scenarios where some of the data are censored.\n",
    "\n",
    "In general, the posterior distributions are wider when we have incomplete data, because less information leads to more uncertainty.\n",
    "\n",
    "This example is based on data I generated; in the next section we'll do a similar analysis with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Bulbs\n",
    "\n",
    "In 2007 [researchers ran an experiment](https://www.researchgate.net/publication/225450325_Renewal_Rate_of_Filament_Lamps_Theory_and_Experiment) to characterize the distribution of lifetimes for light bulbs.\n",
    "\n",
    "Here is their description of the experiment:\n",
    "\n",
    "> An assembly of 50 new Philips (India) lamps with the rating 40 W, 220 V (AC) was taken and installed in the horizontal orientation and uniformly distributed over a lab area 11 m x 7 m.\n",
    ">\n",
    "> The assembly was monitored at regular intervals of 12 h to look for failures. The instants of recorded failures were [recorded] and a total of 32 data points were obtained such that even the last bulb failed. \n",
    "\n",
    "The data were reported in the following columns:\n",
    "\n",
    "```\n",
    "i - observation number\n",
    "h - time in hours since experiment start\n",
    "f - number of failed lamps at particular time h\n",
    "K - number of surviving lamps  at particular time h\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datafile = 'lamps.csv'\n",
    "if not os.path.exists(datafile):\n",
    "    !wget https://gist.github.com/epogrebnyak/7933e16c0ad215742c4c104be4fbdeb1/raw/c932bc5b6aa6317770c4cbf43eb591511fec08f9/lamps.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data into a `DataFrame` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lamps.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column `h` contains the time values when bulbs failed; Column `f` contains the number of bulbs that failed at each time.\n",
    "We can represent these values and frequencies using a `Pmf`, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Pmf\n",
    "\n",
    "pmf_bulb = Pmf(df['f'].to_numpy(), df['h'])\n",
    "pmf_bulb.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the design of this experiment, we can consider the data to be a representative sample from the distribution of lifetimes, at least for light bulbs that are lit continuously.\n",
    "\n",
    "The average lifetime is about 1400 h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_bulb.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming, for now, that these data follow a Weibull distribution, let's estimate the parameters that fit the data.\n",
    "\n",
    "Again, I'll start with uniform priors for $\\lambda$ and $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = np.linspace(1000, 2000, num=51)\n",
    "prior_lam = make_uniform(lams, name='lambda')\n",
    "prior_lam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.linspace(1, 10, num=51)\n",
    "prior_k = make_uniform(ks, name='k')\n",
    "prior_k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, there are 51 values in the prior distribtion, rather than the usual 101.  That's because we are going to use the posterior distributions to do some computationally-intensive calculations.\n",
    "They will run faster with fewer values, but the results will be less precise.\n",
    "\n",
    "As usual, we can use `make_joint` to make the prior joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_bulb = make_joint(prior_lam, prior_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have data for 50 light bulbs, there are only 32 unique lifetimes in the dataset.  For the update, it is convenient to express the data in the form of 50 lifetimes, with each lifetime repeated the given number of times.\n",
    "We can use `np.repeat` to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bulb = np.repeat(df['h'], df['f'])\n",
    "len(data_bulb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `update_weibull` to do the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_bulb = update_weibull(prior_bulb, data_bulb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the posterior joint distribution looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_contour(posterior_bulb)\n",
    "decorate(title='Joint posterior distribution of Weibull parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete Information\n",
    "\n",
    "But that's not quite right, because it assumes each light bulb died at the instant we observed it.  \n",
    "According to the report, the researchers only checked the bulbs every 12 hours.  So if they see that a bulb has died, they know only that it died during the 12 hours since the last check.\n",
    "\n",
    "It is more strictly correct to use the following update function, which uses the CDF of the Weibull distribution to compute the probability that a bulb dies during a given 12 hour interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weibull_between(prior, data, dt=12):\n",
    "    \"\"\"Update the prior based on data.\n",
    "    \n",
    "    prior: joint distribution of mu and sigma\n",
    "    data: sequence of observations\n",
    "    \"\"\"\n",
    "    lam_mesh, k_mesh, data_mesh = np.meshgrid(\n",
    "        prior.columns, prior.index, data)\n",
    "    dist = weibull_dist(lam_mesh, k_mesh)\n",
    "    cdf1 = dist.cdf(data_mesh)\n",
    "    cdf2 = dist.cdf(data_mesh-12)\n",
    "    likelihood = (cdf1 - cdf2).prod(axis=2)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    normalize(posterior)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that a value falls in an interval is the difference between the CDF at the beginning and end of the interval.\n",
    "\n",
    "Here's how we run the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_bulb2 = update_weibull_between(prior_bulb, data_bulb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(posterior_bulb2)\n",
    "decorate(title='Joint posterior distribution of Weibull parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually this result is almost identical to what we got using the PDF.\n",
    "And that's good news, because it suggests that using the PDF can be a good approximation even if it's not strictly correct.\n",
    "\n",
    "To see whether it makes any difference at all, let's check the posterior means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Means\n",
    "\n",
    "To compute the posterior mean of a joint distribution, we'll make a mesh that contains the values of $\\lambda$ and $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_mesh, k_mesh = np.meshgrid(\n",
    "    prior_bulb.columns, prior_bulb.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each pair of parameters we'll use `weibull_dist` to compute the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = weibull_dist(lam_mesh, k_mesh).mean()\n",
    "means.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with the same dimensions as the joint distribution.\n",
    "\n",
    "Now we need to weight each mean with the corresponding probability from the joint posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = means * posterior_bulb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute the sum of the weighted means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.to_numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function encapsulates these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_weibull_mean(joint):\n",
    "    \"\"\"Compute the mean of a joint distribution of Weibulls.\n",
    "    \n",
    "    joint: DataFrame of Weibull parameters and probabilities\n",
    "    \n",
    "    returns: expected mean value\n",
    "    \"\"\"\n",
    "    lam_mesh, k_mesh = np.meshgrid(\n",
    "        joint.columns, joint.index)\n",
    "    means = weibull_dist(lam_mesh, k_mesh).mean()\n",
    "    prod = means * joint\n",
    "    return prod.to_numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the means of the two posterior distributions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_weibull_mean(posterior_bulb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_weibull_mean(posterior_bulb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take into account the 12-hour interval between observations, the posterior mean is about 6 hours less.\n",
    "And that makes sense: if we assume that a bulb is equally likely to expire at any point in the interval, the average would be the midpoint of the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Distribution\n",
    "\n",
    "Suppose you install 100 light bulbs of the kind we studied in this chapter, and you come back to check on them after 1000 hours.  Based on the posterior distribution we just computed, what is the distribution of the number of bulbs you find dead?\n",
    "\n",
    "If we knew the parameters of the Weibull distribution for sure, the answer would be a binomial distribution.\n",
    "\n",
    "For example, if we know that $\\lambda=1550$ and $k=4.25$, we can use `weibull_dist` to compute the probability that a bulb dies before you return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1550\n",
    "k = 4.25\n",
    "t = 1000\n",
    "\n",
    "prob_dead = weibull_dist(lam, k).cdf(t)\n",
    "prob_dead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are 100 bulbs and each has this probability of dying, the number of dead bulbs follows a binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_binomial\n",
    "\n",
    "n = 100\n",
    "p = prob_dead\n",
    "dist_num_dead = make_binomial(n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_num_dead.plot()\n",
    "\n",
    "decorate(xlabel='Number of dead bulbs',\n",
    "         ylabel='PMF',\n",
    "         title='Predictive distribution with known parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's based on the assumption that we know $\\lambda$ and $k$, and we don't.\n",
    "Instead, we have a posterior distribution that contains possible values of these parameters and their probabilities.\n",
    "\n",
    "So the posterior predictive distribution is not a single binomial; instead it is a mixture of binomials, weighted with the posterior probabilities.\n",
    "\n",
    "We can use `make_mixture` to compute the posterior predictive distribution.  \n",
    "It doesn't work with joint distributions, but we can convert the `DataFrame` that represents a joint distribution to a `Series`, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_series = posterior_bulb2.stack()\n",
    "posterior_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a `Series` with a `MultiIndex` that contains two \"levels\": the first level contains the values of `k`; the second contains the values of `lam`.\n",
    "\n",
    "With the posterior in this form, we can iterate through the possible parameters and compute a predictive distribution for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_seq = []\n",
    "for (k, lam) in posterior_series.index:\n",
    "    prob_dead = weibull_dist(lam, k).cdf(t)\n",
    "    pmf = make_binomial(n, prob_dead)\n",
    "    pmf_seq.append(pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `make_mixture`, passing as parameters the posterior probabilities in `posterior_series` and the sequence of binomial distributions in `pmf_seq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_mixture\n",
    "\n",
    "post_pred = make_mixture(posterior_series, pmf_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the posterior predictive distribution looks like, compared to the binomial distribution we computed with known parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_num_dead.plot(label='known parameters')\n",
    "post_pred.plot(label='unknown parameters')\n",
    "decorate(xlabel='Number of dead bulbs',\n",
    "         ylabel='PMF',\n",
    "         title='Posterior predictive distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution is wider because it represents our uncertainty about the parameters as well as our uncertainty about the number of dead bulbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter introduces survival analysis, which is used to answer questions about the time until an event, and the Weibull distribution, which is a good model for \"lifetimes\" (broadly interpreted) in a number of domains.\n",
    "\n",
    "We used joint distributions to represent prior probabilities for the parameters of the Weibull distribution, and we updated them three ways: knowing the exact duration of a lifetime, knowing a lower bound, and knowing that a lifetime fell in a given interval.\n",
    "\n",
    "These examples demonstrate a feature of Bayesian methods: they can be adapted to handle incomplete, or \"censored\", data with only small changes.  As an exercise, you'll have a chance to work with one more type of censored data, when we are given an upper bound on a lifetime.\n",
    "\n",
    "The methods in this chapter work with any distribution with two parameters.  \n",
    "In the exercises, you'll have a chance to estimate the parameters of a two-parameter gamma distribution, which is used to describe a variety of natural phenomena.\n",
    "\n",
    "And in the next chapter we'll move on to models with three parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise:** Using data about the lifetimes of light bulbs, we computed the posterior distribution from the parameters of a Weibull distribution, $\\lambda$ and $k$, and the posterior predictive distribution for the number of dead bulbs, out of 100, after 1000 hours.\n",
    "\n",
    "Now suppose you do the experiment:  You install 100 light bulbs, come back after 1000 hours, and find 20 dead light bulbs.  \n",
    "\n",
    "Update the posterior distribution based on this data.\n",
    "How much does it change the posterior mean?\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "1. Use a mesh grid to compute the probability of finding a bulb dead after 1000 hours for each pair of parameters.\n",
    "\n",
    "2. For each of those probabilities, compute the likelihood of finding 20 dead bulbs out of 100.\n",
    "\n",
    "3. Use those likelihoods to update the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In this exercise, we'll use one month of data to estimate the parameters of a distribution that describes daily rainfall in Seattle.\n",
    "Then we'll compute the posterior predictive distribution for daily rainfall and use it to estimate the probability of a rare event, like more than 1.5 inches of rain in a day.\n",
    "\n",
    "According to hydrologists, the distribution of total daily rainfall (for days with rain) is well modeled by a two-parameter\n",
    "gamma distribution.\n",
    "\n",
    "When we worked with the one-parameter gamma distribution in Chapter xx, we used the Greek letter $\\alpha$ for the parameter.\n",
    "\n",
    "For the two-parameter gamma distribution, we will use $k$ for the \"shape parameter\", which determines the shape of the distribution, and the Greek letter $\\theta$ or `theta` for the \"scale parameter\". \n",
    "\n",
    "The following function takes these parameters and returns a `gamma` object from SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def gamma_dist(k, theta):\n",
    "    \"\"\"Makes a gamma object.\n",
    "    \n",
    "    k: shape parameter\n",
    "    theta: scale parameter\n",
    "    \n",
    "    returns: gamma object\n",
    "    \"\"\"\n",
    "    return scipy.stats.gamma(k, scale=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some data.\n",
    "The following cell downloads data I collected from the National Oceanic and Atmospheric Administration ([NOAA](http://www.ncdc.noaa.gov/cdo-web/search)) for Seattle, Washington in May 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "\n",
    "datafile = '2203951.csv'\n",
    "if not os.path.exists(datafile):\n",
    "    !wget https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2203951.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it into a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('2203951.csv')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll make a Boolean Series to indicate which days it rained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rained = weather['PRCP'] > 0\n",
    "rained.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And select the total rainfall on the days it rained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp = weather.loc[rained, 'PRCP']\n",
    "prcp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the CDF of the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_data = Cdf.from_seq(prcp)\n",
    "cdf_data.plot()\n",
    "decorate(xlabel='Total rainfall (in)',\n",
    "         ylabel='CDF',\n",
    "         title='Distribution of rainfall on days it rained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum is 1.14 inches of rain is one day.\n",
    "To estimate the probability of more than 1.5 inches, we need to extrapolate from the data we have, so our estimate will depend on whether the gamma distribution is really a good model.\n",
    "\n",
    "I suggest you proceed in the following steps:\n",
    "\n",
    "1. Construct a prior distribution for the parameters of the gamma distribution.  Note that $k$ and $\\theta$ must be greater than 0.\n",
    "\n",
    "2. Use the observed rainfalls to update the distribution of parameters.\n",
    "\n",
    "3. Compute the posterior predictive distribution of rainfall, and use it to estimate the probability of getting more than 1.5 inches of rain in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
